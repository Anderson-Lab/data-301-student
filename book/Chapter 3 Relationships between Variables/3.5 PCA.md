---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.1'
      jupytext_version: 1.2.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Chapter 3.5 - Principal Component Analysis (alpha)
When examining data viewing the correlation can be informative, and often understanding the relationships between variables is a large part of the overall data science project; however, the covariance matrix has other uses in data science. Keep in mind that the covariance matrix captures the magnitude and direction of the relationship between variables. This can be used to perform dimensionality reduction or as a preprocessing step for machine learning. In this chapter, we will discuss one such dimensionality reduction method - **Principal Component Analysis** or PCA.

Let's take a look at the Ames Housing data set.

```python
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

housing_df = pd.read_csv("https://raw.githubusercontent.com/dlsun/data-science-book/master/data/AmesHousing.txt", sep="\t")
housing_df.head()
```

A lot of this data is numeric but not all of the data. For PCA, we must have numeric data because it operates on the covariance matrix which requires numeric data. Let's take a look at the data types to see which columns are candidates for this analysis.

```python
housing_df_numeric = housing_df.loc[:,housing_df.dtypes==np.int64]
```

```python
housing_df_numeric.head()
```

There are still a lot of numeric columns that don't make much sense to include for PCA. 


**Why would we not want to include the PID in a PCA analysis?**

For the sake of this example, let's pick four variables: ``Year Built``, ``SalePrice``, ``Lot Area``, ``1st Flr SF``

```python
housing_df_numeric.loc[:,['Year Built', 'SalePrice', 'Lot Area', '1st Flr SF']]
```

## sklearn and PCA
sklearn is another really popular package that provides many useful prebuilt modules and functions. One of those is an implementation of PCA. The common structure to a sklearn use case is to create an object (PCA), then call ``fit`` on data and then either a call to ``transform`` or ``predict``.


**Dimensionality Reduction**

Our first goal for PCA is to use it to visualize more than two dimensions in two dimensions.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca.fit(housing_df_numeric.loc[:,['Year Built', 'SalePrice', 'Lot Area', '1st Flr SF']])

X = pca.transform(housing_df_numeric.loc[:,['Year Built', 'SalePrice', 'Lot Area', '1st Flr SF']])
```

```python
X
```

**How does it transform these old dimensions into new ones?**

It uses the vectors (components) found during the fit procedure. The new dimensions are constructed by taking the dot product between the original data and the components.

```python
pca.components_
```

```python
pca.mean_
```

```python
pca.components_[0,:].dot(housing_df_numeric.loc[:,['Year Built', 'SalePrice', 'Lot Area', '1st Flr SF']].iloc[0,:]-pca.mean_)
```

```python
X.shape,housing_df.shape
```

**It matches!!!** The only extra bit was subtracting the mean to center the data. In order to plot things easily, we want to put the X data into a dataframe.

```python
Xdf = pd.DataFrame(X,columns=["PC1","PC2"])
df = housing_df.join(Xdf) 
```

You'll notice above that we did a new function called a join. That join allows us to join two dataframes by their index. We need this so we can include some of the other original columns that were not numeric. Now we want to take a look at all our work. We will use the altair package and plot PC1 and PC2.

```python
from altair import * 
```

```python
Chart(df).mark_point().encode(x="PC1",y="PC2",color="Lot Shape",size="Lot Area")
```

What if we scale the data?

```python
from sklearn.preprocessing import StandardScaler
# Separating out the features
x = housing_df_numeric.loc[:,['Year Built', 'SalePrice', 'Lot Area', '1st Flr SF']].values
# Standardizing the features
x = StandardScaler().fit_transform(x)

pca = PCA(n_components=2)
X = pca.fit_transform(x)
Xdf = pd.DataFrame(X,columns=["PC1","PC2"])
df = housing_df.join(Xdf) 

Chart(df).mark_point().encode(x="PC1",y="PC2",color="Lot Shape",size="Lot Area")
```

Notice that this is not just a scaling of the axes when comparing to the previous plot.


# Exercises


Exercises 1-3 deal with the Tips data set (`https://raw.githubusercontent.com/dlsun/data-science-book/master/data/tips.csv`).


**Exercise 1.** Construct a PCA plot with two components from ``total_bill``, ``tip``, and ``size``.

```python
# TYPE YOUR CODE HERE
```

**Exercise 2.** Construct the same plot but scale the data.

```python
# TYPE YOUR CODE HERE
```

**Exercise 3.** After playing around with different ways to color or size, are there any patterns that seem to appear?


# TYPE YOUR ANSWER HERE

```python

```
